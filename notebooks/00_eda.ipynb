{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a09fb3a",
   "metadata": {},
   "source": [
    "# M-TRI Exploratory Data Analysis\n",
    "\n",
    "This notebook performs initial data exploration for the Microbial Toxin-Risk Index (M-TRI) project. We analyze pond characteristics, temporal patterns, spatial coverage, and data quality to understand what we're working with before building models.\n",
    "\n",
    "**Critical checks:**\n",
    "- Data coverage across time and space\n",
    "- Label balance (toxin vs non-toxin events)  \n",
    "- Missing data patterns\n",
    "- Spatial distribution of monitoring sites\n",
    "- Quality issues that need preprocessing\n",
    "\n",
    "If major data problems appear here, we need to fix ingestion before proceeding to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f734e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium import plugins\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff2a02",
   "metadata": {},
   "source": [
    "## Load Sample Dataset\n",
    "\n",
    "Loading our sample dataset to check data structure and quality. This sample contains 25 pond observations across multiple dates in New Jersey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "data_path = \"../data/sample/merged_features.csv\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"ERROR: Sample data not found at {data_path}\")\n",
    "    print(\"Make sure you're running this from the notebooks/ directory\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"Unique ponds: {df['pond_id'].nunique()}\")\n",
    "    \n",
    "    # Convert date column\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfffa47",
   "metadata": {},
   "source": [
    "## Dataset Overview and Basic Statistics\n",
    "\n",
    "Let's understand the basic structure, data types, and statistical properties of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total observations: {len(df)}\")\n",
    "print(f\"Features: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== BASIC STATISTICS ===\")\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Display descriptive statistics for numeric columns\n",
    "print(\"\\nNumeric features summary:\")\n",
    "display(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368610db",
   "metadata": {},
   "source": [
    "## Coverage Tables Analysis\n",
    "\n",
    "Analyzing data availability across features, time periods, and geographic regions. This helps identify gaps that could bias our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5532e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage analysis\n",
    "print(\"=== DATA COVERAGE ANALYSIS ===\")\n",
    "\n",
    "# 1. Feature completeness\n",
    "print(\"\\n1. Feature Completeness (% non-null values):\")\n",
    "feature_coverage = (df.notna().sum() / len(df) * 100).round(1)\n",
    "coverage_df = pd.DataFrame({\n",
    "    'Feature': feature_coverage.index,\n",
    "    'Coverage_%': feature_coverage.values,\n",
    "    'Missing_Count': df.isnull().sum().values\n",
    "}).sort_values('Coverage_%')\n",
    "\n",
    "print(coverage_df.to_string(index=False))\n",
    "\n",
    "# 2. Temporal coverage\n",
    "print(f\"\\n2. Temporal Coverage:\")\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "temporal_coverage = df.groupby(['year', 'month']).size().reset_index(name='observations')\n",
    "print(\"Observations per month:\")\n",
    "print(temporal_coverage.to_string(index=False))\n",
    "\n",
    "# 3. Spatial coverage  \n",
    "print(f\"\\n3. Spatial Coverage:\")\n",
    "lat_range = df['lat'].max() - df['lat'].min()\n",
    "lon_range = df['lon'].max() - df['lon'].min()\n",
    "print(f\"Latitude range: {df['lat'].min():.4f} to {df['lat'].max():.4f} (span: {lat_range:.4f}¬∞)\")\n",
    "print(f\"Longitude range: {df['lon'].min():.4f} to {df['lon'].max():.4f} (span: {lon_range:.4f}¬∞)\")\n",
    "print(f\"Unique pond locations: {df[['lat', 'lon']].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# 4. Key feature availability\n",
    "key_features = ['chlorophyll_proxy_14d', 'phosphate_mean_7d', 'nitrate_mean_7d', \n",
    "                'eDNA_mcy_detected', 'toxin_detected']\n",
    "print(f\"\\n4. Key Feature Availability:\")\n",
    "for feature in key_features:\n",
    "    if feature in df.columns:\n",
    "        coverage = (df[feature].notna().sum() / len(df)) * 100\n",
    "        print(f\"{feature}: {coverage:.1f}% complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf773903",
   "metadata": {},
   "source": [
    "## Temporal Data Analysis and Plots\n",
    "\n",
    "Examining how our data changes over time and identifying seasonal patterns that might affect toxin risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis and visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Observations over time\n",
    "obs_by_date = df.groupby('date').size()\n",
    "axes[0,0].plot(obs_by_date.index, obs_by_date.values, 'o-')\n",
    "axes[0,0].set_title('Observations per Date')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Number of Observations')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Chlorophyll levels over time\n",
    "axes[0,1].plot(df['date'], df['chlorophyll_proxy_14d'], 'o', alpha=0.6)\n",
    "axes[0,1].set_title('Chlorophyll Proxy Over Time')\n",
    "axes[0,1].set_xlabel('Date')\n",
    "axes[0,1].set_ylabel('Chlorophyll Proxy (14d avg)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Toxin detection by month\n",
    "toxin_by_month = df.groupby(df['date'].dt.month)['toxin_detected'].agg(['sum', 'count'])\n",
    "toxin_rate = toxin_by_month['sum'] / toxin_by_month['count'] * 100\n",
    "\n",
    "axes[1,0].bar(toxin_rate.index, toxin_rate.values, alpha=0.7)\n",
    "axes[1,0].set_title('Toxin Detection Rate by Month')\n",
    "axes[1,0].set_xlabel('Month')\n",
    "axes[1,0].set_ylabel('Detection Rate (%)')\n",
    "axes[1,0].set_xticks(range(1, 13))\n",
    "\n",
    "# 4. Key nutrients over time\n",
    "axes[1,1].scatter(df['date'], df['phosphate_mean_7d'], alpha=0.6, label='Phosphate', s=30)\n",
    "axes[1,1].scatter(df['date'], df['nitrate_mean_7d'], alpha=0.6, label='Nitrate', s=30)\n",
    "axes[1,1].set_title('Nutrient Levels Over Time')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Concentration')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics by time period\n",
    "print(\"\\n=== TEMPORAL PATTERNS ===\")\n",
    "print(\"Toxin detection by month:\")\n",
    "monthly_stats = df.groupby(df['date'].dt.month).agg({\n",
    "    'toxin_detected': ['count', 'sum', 'mean'],\n",
    "    'chlorophyll_proxy_14d': 'mean',\n",
    "    'phosphate_mean_7d': 'mean'\n",
    "}).round(3)\n",
    "print(monthly_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930b25c",
   "metadata": {},
   "source": [
    "## Label Balance Analysis\n",
    "\n",
    "Critical analysis of our target variable (toxin_detected). Imbalanced datasets need special handling during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label balance analysis\n",
    "print(\"=== LABEL BALANCE ANALYSIS ===\")\n",
    "\n",
    "# Basic label distribution\n",
    "label_counts = df['toxin_detected'].value_counts()\n",
    "label_pct = df['toxin_detected'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Total observations: {len(df)}\")\n",
    "print(f\"Toxin detected (1): {label_counts.get(1, 0)} ({label_pct.get(1, 0):.1f}%)\")\n",
    "print(f\"No toxin (0): {label_counts.get(0, 0)} ({label_pct.get(0, 0):.1f}%)\")\n",
    "\n",
    "# Check for severe imbalance\n",
    "minority_class_pct = min(label_pct.values) if len(label_pct) > 1 else 0\n",
    "if minority_class_pct < 10:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Severe class imbalance detected ({minority_class_pct:.1f}% minority class)\")\n",
    "    print(\"   Consider: SMOTE, class weights, or different evaluation metrics\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Bar chart\n",
    "label_counts.plot(kind='bar', ax=axes[0], color=['lightcoral', 'skyblue'])\n",
    "axes[0].set_title('Label Distribution (Count)')\n",
    "axes[0].set_xlabel('Toxin Detected')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Pie chart  \n",
    "axes[1].pie(label_counts.values, labels=[f'No Toxin\\n({label_counts.get(0, 0)})', \n",
    "                                        f'Toxin Detected\\n({label_counts.get(1, 0)})'],\n",
    "            autopct='%1.1f%%', colors=['lightcoral', 'skyblue'])\n",
    "axes[1].set_title('Label Distribution (%)')\n",
    "\n",
    "# 3. Label distribution by key feature\n",
    "if 'eDNA_mcy_detected' in df.columns:\n",
    "    cross_tab = pd.crosstab(df['eDNA_mcy_detected'], df['toxin_detected'], normalize='columns') * 100\n",
    "    cross_tab.plot(kind='bar', ax=axes[2], color=['lightcoral', 'skyblue'])\n",
    "    axes[2].set_title('Toxin Detection by eDNA Evidence')\n",
    "    axes[2].set_xlabel('eDNA mcy Gene Detected')\n",
    "    axes[2].set_ylabel('Percentage')\n",
    "    axes[2].legend(['No Toxin', 'Toxin Detected'])\n",
    "    axes[2].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional label analysis\n",
    "print(\"\\n=== LABEL CORRELATION WITH KEY FEATURES ===\")\n",
    "corr_features = ['chlorophyll_proxy_14d', 'phosphate_mean_7d', 'nitrate_mean_7d', \n",
    "                'eDNA_mcy_detected', 'ndvi_mean_14d']\n",
    "\n",
    "correlations = []\n",
    "for feature in corr_features:\n",
    "    if feature in df.columns:\n",
    "        corr = df[feature].corr(df['toxin_detected'])\n",
    "        correlations.append((feature, corr))\n",
    "        \n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "print(\"Features most correlated with toxin detection:\")\n",
    "for feature, corr in correlations:\n",
    "    print(f\"{feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec8098",
   "metadata": {},
   "source": [
    "## Missingness Heatmap Visualization\n",
    "\n",
    "Visualizing patterns in missing data to understand systematic gaps in our monitoring coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data visualization\n",
    "print(\"=== MISSING DATA PATTERNS ===\")\n",
    "\n",
    "# Calculate missing data percentages\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"Missing data percentages by feature:\")\n",
    "for col in missing_pct.index:\n",
    "    if missing_pct[col] > 0:\n",
    "        print(f\"{col}: {missing_pct[col]:.1f}%\")\n",
    "\n",
    "# Create missingness heatmap\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# 1. Overall missingness heatmap\n",
    "missing_matrix = df.isnull()\n",
    "sns.heatmap(missing_matrix.T, \n",
    "           cbar=True, \n",
    "           ax=axes[0],\n",
    "           cmap='RdYlBu_r',\n",
    "           xticklabels=False,\n",
    "           yticklabels=True)\n",
    "axes[0].set_title('Missing Data Heatmap (Red = Missing)')\n",
    "axes[0].set_xlabel('Observation Index')\n",
    "\n",
    "# 2. Missing data correlation matrix\n",
    "# Show which features tend to be missing together\n",
    "missing_corr = missing_matrix.corr()\n",
    "mask = np.triu(np.ones_like(missing_corr, dtype=bool))  # Hide upper triangle\n",
    "sns.heatmap(missing_corr, \n",
    "           mask=mask,\n",
    "           annot=True, \n",
    "           cmap='RdYlBu_r', \n",
    "           center=0,\n",
    "           square=True,\n",
    "           ax=axes[1],\n",
    "           cbar_kws={\"shrink\": .8})\n",
    "axes[1].set_title('Missing Data Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Missing data patterns by date\n",
    "print(\"\\n=== MISSING DATA BY TIME PERIOD ===\")\n",
    "df_with_missing = df.copy()\n",
    "df_with_missing['missing_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "missing_by_date = df_with_missing.groupby('date')['missing_count'].agg(['mean', 'max']).round(2)\n",
    "print(\"Average and max missing features per observation by date:\")\n",
    "print(missing_by_date)\n",
    "\n",
    "# Identify problematic features\n",
    "high_missing_features = missing_pct[missing_pct > 20].index.tolist()\n",
    "if high_missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è  Features with >20% missing data: {high_missing_features}\")\n",
    "    print(\"Consider: imputation, feature removal, or alternative data sources\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Good news: No features have >20% missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931c347",
   "metadata": {},
   "source": [
    "## Spatial Coverage Mapping\n",
    "\n",
    "Interactive map showing where our ponds are located and their toxin detection status. This helps identify spatial biases in monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial analysis and mapping\n",
    "print(\"=== SPATIAL COVERAGE ANALYSIS ===\")\n",
    "\n",
    "# Get unique pond locations\n",
    "pond_locations = df.groupby('pond_id').agg({\n",
    "    'lat': 'first',\n",
    "    'lon': 'first', \n",
    "    'toxin_detected': ['sum', 'count'],\n",
    "    'pond_area_m2': 'first'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "pond_locations.columns = ['lat', 'lon', 'toxin_events', 'total_obs', 'area_m2']\n",
    "pond_locations['toxin_rate'] = pond_locations['toxin_events'] / pond_locations['total_obs']\n",
    "\n",
    "print(f\"Total unique ponds: {len(pond_locations)}\")\n",
    "print(f\"Average observations per pond: {pond_locations['total_obs'].mean():.1f}\")\n",
    "print(f\"Ponds with any toxin detection: {(pond_locations['toxin_events'] > 0).sum()}\")\n",
    "\n",
    "# Create interactive map\n",
    "center_lat = df['lat'].mean()\n",
    "center_lon = df['lon'].mean()\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
    "\n",
    "# Add pond markers\n",
    "for pond_id, row in pond_locations.iterrows():\n",
    "    # Color based on toxin detection rate\n",
    "    if row['toxin_rate'] == 0:\n",
    "        color = 'green'\n",
    "        icon = 'ok-sign'\n",
    "    elif row['toxin_rate'] < 0.5:\n",
    "        color = 'orange' \n",
    "        icon = 'warning-sign'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        icon = 'exclamation-sign'\n",
    "    \n",
    "    # Marker size based on pond area\n",
    "    radius = max(5, min(20, row['area_m2'] / 2000))\n",
    "    \n",
    "    popup_text = f\"\"\"\n",
    "    Pond ID: {pond_id}<br>\n",
    "    Observations: {row['total_obs']}<br>\n",
    "    Toxin Events: {row['toxin_events']}<br>\n",
    "    Detection Rate: {row['toxin_rate']:.1%}<br>\n",
    "    Area: {row['area_m2']:,} m¬≤\n",
    "    \"\"\"\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        radius=radius,\n",
    "        popup=popup_text,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fillColor=color,\n",
    "        fillOpacity=0.6\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; \n",
    "     top: 10px; right: 10px; width: 200px; height: 120px; \n",
    "     background-color: white; border:2px solid grey; z-index:9999; \n",
    "     font-size:14px; padding: 10px\">\n",
    "<b>Toxin Detection Rate</b><br>\n",
    "<i class=\"fa fa-circle\" style=\"color:green\"></i> No toxin detected<br>\n",
    "<i class=\"fa fa-circle\" style=\"color:orange\"></i> 1-50% detection<br>\n",
    "<i class=\"fa fa-circle\" style=\"color:red\"></i> >50% detection<br>\n",
    "<br><b>Circle size:</b> Pond area\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Display map\n",
    "display(m)\n",
    "\n",
    "# Spatial statistics\n",
    "print(f\"\\n=== SPATIAL STATISTICS ===\")\n",
    "print(f\"Latitude range: {df['lat'].min():.4f}¬∞ to {df['lat'].max():.4f}¬∞\")\n",
    "print(f\"Longitude range: {df['lon'].min():.4f}¬∞ to {df['lon'].max():.4f}¬∞\")\n",
    "print(f\"Geographic span: ~{lat_range*111:.1f} km N-S, ~{lon_range*85:.1f} km E-W\")\n",
    "\n",
    "# Check for spatial clustering\n",
    "from scipy.spatial.distance import pdist\n",
    "distances = pdist(pond_locations[['lat', 'lon']].values)\n",
    "print(f\"Average distance between ponds: {np.mean(distances)*111:.1f} km\")\n",
    "print(f\"Minimum distance between ponds: {np.min(distances)*111:.1f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b01cfc",
   "metadata": {},
   "source": [
    "## Before/After Thumbnail Comparisons\n",
    "\n",
    "Visual comparisons showing pond conditions during different toxin states. These help validate that our features capture meaningful changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be058e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before/After thumbnail analysis\n",
    "print(\"=== BEFORE/AFTER CONDITION ANALYSIS ===\")\n",
    "\n",
    "# Find ponds with both toxin and non-toxin observations\n",
    "pond_transitions = df.groupby('pond_id')['toxin_detected'].agg(['min', 'max', 'mean'])\n",
    "transitional_ponds = pond_transitions[(pond_transitions['min'] == 0) & (pond_transitions['max'] == 1)]\n",
    "\n",
    "print(f\"Ponds with both clean and toxin periods: {len(transitional_ponds)}\")\n",
    "\n",
    "if len(transitional_ponds) >= 3:\n",
    "    # Select 3 example ponds for before/after comparison\n",
    "    example_ponds = transitional_ponds.head(3).index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    fig.suptitle('Before/After Condition Comparisons (3 Example Ponds)', fontsize=16)\n",
    "    \n",
    "    for i, pond_id in enumerate(example_ponds):\n",
    "        pond_data = df[df['pond_id'] == pond_id].sort_values('date')\n",
    "        \n",
    "        # Get before (no toxin) and after (toxin) conditions\n",
    "        before = pond_data[pond_data['toxin_detected'] == 0].iloc[0] if len(pond_data[pond_data['toxin_detected'] == 0]) > 0 else None\n",
    "        after = pond_data[pond_data['toxin_detected'] == 1].iloc[0] if len(pond_data[pond_data['toxin_detected'] == 1]) > 0 else None\n",
    "        \n",
    "        if before is not None and after is not None:\n",
    "            # Feature comparison bars\n",
    "            features_to_compare = ['chlorophyll_proxy_14d', 'phosphate_mean_7d', \n",
    "                                 'nitrate_mean_7d', 'ndvi_mean_14d']\n",
    "            \n",
    "            for j, feature in enumerate(features_to_compare):\n",
    "                if feature in df.columns:\n",
    "                    before_val = before[feature]\n",
    "                    after_val = after[feature] \n",
    "                    \n",
    "                    axes[i,j].bar(['Before\\n(No Toxin)', 'After\\n(Toxin)'], \n",
    "                                [before_val, after_val],\n",
    "                                color=['lightblue', 'salmon'])\n",
    "                    axes[i,j].set_title(f'Pond {pond_id}\\n{feature}')\n",
    "                    axes[i,j].set_ylabel('Value')\n",
    "                    \n",
    "                    # Add value labels on bars\n",
    "                    axes[i,j].text(0, before_val + 0.01*max(before_val, after_val), \n",
    "                                 f'{before_val:.2f}', ha='center')\n",
    "                    axes[i,j].text(1, after_val + 0.01*max(before_val, after_val), \n",
    "                                 f'{after_val:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical comparison\n",
    "    print(\"\\n=== BEFORE/AFTER STATISTICAL COMPARISON ===\")\n",
    "    before_conditions = df[df['toxin_detected'] == 0]\n",
    "    after_conditions = df[df['toxin_detected'] == 1]\n",
    "    \n",
    "    comparison_features = ['chlorophyll_proxy_14d', 'phosphate_mean_7d', \n",
    "                          'nitrate_mean_7d', 'turbidity_latest', 'ndvi_mean_14d']\n",
    "    \n",
    "    print(\"Average values comparison:\")\n",
    "    print(f\"{'Feature':<25} {'No Toxin':<12} {'Toxin':<12} {'Change':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for feature in comparison_features:\n",
    "        if feature in df.columns:\n",
    "            before_mean = before_conditions[feature].mean()\n",
    "            after_mean = after_conditions[feature].mean()\n",
    "            change_pct = ((after_mean - before_mean) / before_mean * 100) if before_mean != 0 else 0\n",
    "            \n",
    "            print(f\"{feature:<25} {before_mean:<12.3f} {after_mean:<12.3f} {change_pct:+.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Not enough transitional ponds for before/after analysis\")\n",
    "    print(\"Creating synthetic comparison based on available data...\")\n",
    "    \n",
    "    # Show distributions instead\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    key_features = ['chlorophyll_proxy_14d', 'phosphate_mean_7d', 'nitrate_mean_7d']\n",
    "    \n",
    "    for i, feature in enumerate(key_features):\n",
    "        if feature in df.columns:\n",
    "            no_toxin = df[df['toxin_detected'] == 0][feature]\n",
    "            toxin = df[df['toxin_detected'] == 1][feature]\n",
    "            \n",
    "            axes[i].hist(no_toxin, alpha=0.7, label='No Toxin', bins=10, color='lightblue')\n",
    "            axes[i].hist(toxin, alpha=0.7, label='Toxin', bins=10, color='salmon')\n",
    "            axes[i].set_title(f'{feature}\\nDistribution Comparison')\n",
    "            axes[i].set_xlabel('Value')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5b55c",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "Final assessment of data quality issues and recommendations for preprocessing before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3310af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment and recommendations\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "# 1. Outlier detection\n",
    "print(\"\\n1. OUTLIER DETECTION\")\n",
    "numeric_features = ['chlorophyll_proxy_14d', 'phosphate_mean_7d', 'nitrate_mean_7d', \n",
    "                   'turbidity_latest', 'ndvi_mean_14d', 'pond_area_m2']\n",
    "\n",
    "outliers_summary = {}\n",
    "for feature in numeric_features:\n",
    "    if feature in df.columns:\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "        outliers_summary[feature] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': len(outliers) / len(df) * 100,\n",
    "            'range': f\"[{df[feature].min():.3f}, {df[feature].max():.3f}]\"\n",
    "        }\n",
    "\n",
    "for feature, stats in outliers_summary.items():\n",
    "    print(f\"{feature}: {stats['count']} outliers ({stats['percentage']:.1f}%) - Range: {stats['range']}\")\n",
    "\n",
    "# 2. Data consistency checks  \n",
    "print(f\"\\n2. DATA CONSISTENCY CHECKS\")\n",
    "\n",
    "# Check for impossible values\n",
    "issues = []\n",
    "if 'ndvi_mean_14d' in df.columns:\n",
    "    invalid_ndvi = df[(df['ndvi_mean_14d'] < -1) | (df['ndvi_mean_14d'] > 1)]\n",
    "    if len(invalid_ndvi) > 0:\n",
    "        issues.append(f\"Invalid NDVI values (should be -1 to 1): {len(invalid_ndvi)} cases\")\n",
    "\n",
    "if 'pond_area_m2' in df.columns:\n",
    "    zero_area = df[df['pond_area_m2'] <= 0]\n",
    "    if len(zero_area) > 0:\n",
    "        issues.append(f\"Zero or negative pond areas: {len(zero_area)} cases\")\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicates = df.duplicated(subset=['pond_id', 'date'])\n",
    "if duplicates.sum() > 0:\n",
    "    issues.append(f\"Duplicate pond-date combinations: {duplicates.sum()} cases\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"‚ö†Ô∏è  {issue}\")\n",
    "else:\n",
    "    print(\"‚úÖ No obvious data consistency issues found\")\n",
    "\n",
    "# 3. Feature relationships\n",
    "print(f\"\\n3. FEATURE RELATIONSHIP ANALYSIS\")\n",
    "correlation_matrix = df[numeric_features].corr()\n",
    "\n",
    "# Find highly correlated features (potential redundancy)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = abs(correlation_matrix.iloc[i, j])\n",
    "        if corr_val > 0.8:  # High correlation threshold\n",
    "            feat1 = correlation_matrix.columns[i]\n",
    "            feat2 = correlation_matrix.columns[j] \n",
    "            high_corr_pairs.append((feat1, feat2, corr_val))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (>0.8):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"‚úÖ No highly correlated features found\")\n",
    "\n",
    "# 4. Recommendations\n",
    "print(f\"\\n=== PREPROCESSING RECOMMENDATIONS ===\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Missing data\n",
    "missing_pct = df.isnull().sum() / len(df) * 100\n",
    "high_missing = missing_pct[missing_pct > 10]\n",
    "if len(high_missing) > 0:\n",
    "    recommendations.append(f\"Handle missing data in: {list(high_missing.index)}\")\n",
    "\n",
    "# Class imbalance  \n",
    "toxin_pct = df['toxin_detected'].mean() * 100\n",
    "if toxin_pct < 20 or toxin_pct > 80:\n",
    "    recommendations.append(f\"Address class imbalance (toxin rate: {toxin_pct:.1f}%)\")\n",
    "\n",
    "# Outliers\n",
    "high_outlier_features = [f for f, stats in outliers_summary.items() if stats['percentage'] > 5]\n",
    "if high_outlier_features:\n",
    "    recommendations.append(f\"Consider outlier handling for: {high_outlier_features}\")\n",
    "\n",
    "# Feature scaling\n",
    "feature_ranges = {}\n",
    "for feature in numeric_features:\n",
    "    if feature in df.columns:\n",
    "        feature_ranges[feature] = df[feature].max() - df[feature].min()\n",
    "\n",
    "max_range = max(feature_ranges.values())\n",
    "min_range = min(feature_ranges.values()) \n",
    "if max_range / min_range > 100:\n",
    "    recommendations.append(\"Apply feature scaling (wide range differences detected)\")\n",
    "\n",
    "# Final recommendations\n",
    "if recommendations:\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "else:\n",
    "    print(\"‚úÖ Data quality looks good for model training!\")\n",
    "\n",
    "print(f\"\\n=== EDA SUMMARY ===\")\n",
    "print(f\"‚úÖ Dataset loaded: {len(df)} observations, {len(df.columns)} features\")\n",
    "print(f\"‚úÖ Spatial coverage: {df['pond_id'].nunique()} unique ponds\")\n",
    "print(f\"‚úÖ Temporal coverage: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"‚úÖ Target balance: {df['toxin_detected'].mean()*100:.1f}% positive cases\")\n",
    "print(f\"‚úÖ Missing data: {(df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100):.1f}% overall\")\n",
    "\n",
    "if len(recommendations) == 0:\n",
    "    print(f\"\\nüéØ Data quality is sufficient - ready to proceed with feature engineering and modeling!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Address {len(recommendations)} data quality issues before modeling\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
